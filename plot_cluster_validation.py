#!/usr/bin/env python3
"""
Plot cluster validation metrics (elbow and silhouette) from parallel clustering results.

This script reads the k*_results.txt files generated by the parallel voxel clustering
workflow and creates clean elbow and silhouette plots for cluster validation.

Usage:
    python plot_cluster_validation.py --results_dir /path/to/results --output_dir /path/to/plots
    python plot_cluster_validation.py --results_dir derivatives/voxel_clustering
"""

import argparse
import os
import os.path as op
import re
import glob
import numpy as np
import matplotlib.pyplot as plt


def get_parser():
    """Command line argument parser"""
    p = argparse.ArgumentParser(description="Plot cluster validation metrics")
    p.add_argument(
        "--results_dir",
        required=True,
        help="Directory containing k*_results.txt files",
    )
    p.add_argument(
        "--output_dir",
        help="Directory to save plots (default: results_dir/validation_plots)",
    )
    p.add_argument(
        "--k_min",
        type=int,
        default=2,
        help="Minimum k value to plot",
    )
    p.add_argument(
        "--k_max",
        type=int,
        default=10,
        help="Maximum k value to plot",
    )
    p.add_argument(
        "--figsize",
        nargs=2,
        type=float,
        default=[12, 5],
        help="Figure size (width height)",
    )
    p.add_argument(
        "--dpi",
        type=int,
        default=300,
        help="Figure DPI for saved plots",
    )
    return p


def parse_results_file(filepath):
    """Parse a k*_results.txt file to extract metrics"""
    metrics = {}

    try:
        with open(filepath, "r") as f:
            content = f.read()

        # Extract k value from filename
        k_match = re.search(r"k(\d+)_results\.txt", op.basename(filepath))
        if k_match:
            k_value = int(k_match.group(1))
            metrics["k"] = k_value

        # Extract method
        method_match = re.search(r"Method:\s*(\w+)", content)
        if method_match:
            metrics["method"] = method_match.group(1)

        # Extract silhouette score
        sil_match = re.search(r"Silhouette score:\s*([\d\.-]+)", content)
        if sil_match:
            metrics["silhouette"] = float(sil_match.group(1))

        # Extract gap statistic
        gap_match = re.search(r"gap_statistic:\s*([\d\.-]+)", content)
        if gap_match:
            metrics["gap_statistic"] = float(gap_match.group(1))

        # Extract gap standard error
        gap_std_match = re.search(r"gap_std:\s*([\d\.-]+)", content)
        if gap_std_match:
            metrics["gap_std"] = float(gap_std_match.group(1))

        # Extract group sizes
        groups_match = re.search(r"Participants per group:\s*\[(.*?)\]", content)
        if groups_match:
            group_sizes = [int(x.strip()) for x in groups_match.group(1).split()]
            metrics["group_sizes"] = group_sizes

    except Exception as e:
        print(f"Warning: Could not parse {filepath}: {e}")

    return metrics


def collect_all_results(results_dir, k_min=2, k_max=10):
    """Collect results from all k*_results.txt files"""
    print(f"Collecting results from: {results_dir}")

    # Find all k*_results.txt files
    pattern = op.join(results_dir, "k*_results.txt")
    result_files = glob.glob(pattern)

    if not result_files:
        raise FileNotFoundError(f"No k*_results.txt files found in {results_dir}")

    print(f"Found {len(result_files)} result files")

    all_metrics = []
    for filepath in result_files:
        metrics = parse_results_file(filepath)
        if metrics and "k" in metrics:
            # Filter by k range
            if k_min <= metrics["k"] <= k_max:
                all_metrics.append(metrics)
                sil_score = metrics.get("silhouette", "N/A")
                gap_val = metrics.get("gap_statistic", "N/A")
                print(f"  k={metrics['k']}: silhouette={sil_score}, gap={gap_val}")

    # Sort by k value
    all_metrics.sort(key=lambda x: x["k"])
    return all_metrics


def plot_validation_metrics(all_metrics, output_dir, figsize=(12, 5), dpi=300):
    """Create elbow and silhouette validation plots"""
    if not all_metrics:
        raise ValueError("No valid metrics found to plot")

    print("Creating validation plots...")

    # Extract data
    k_values = [m["k"] for m in all_metrics]
    silhouette_scores = [m.get("silhouette", np.nan) for m in all_metrics]

    # Check if we have silhouette scores
    has_silhouette = not all(np.isnan(silhouette_scores))

    # Create figure
    if has_silhouette:
        fig, axes = plt.subplots(1, 2, figsize=figsize)
    else:
        fig, axes = plt.subplots(1, 1, figsize=(figsize[0] / 2, figsize[1]))
        axes = [axes]

    # Plot 1: Silhouette Analysis
    if has_silhouette:
        # Remove NaN values for plotting
        valid_indices = ~np.isnan(silhouette_scores)
        valid_k = np.array(k_values)[valid_indices]
        valid_sil = np.array(silhouette_scores)[valid_indices]

        axes[0].plot(valid_k, valid_sil, "o-", linewidth=2, markersize=6, color="blue")
        axes[0].set_xlabel("Number of Clusters (k)")
        axes[0].set_ylabel("Silhouette Score")
        axes[0].set_title("Silhouette Analysis")
        axes[0].grid(True, alpha=0.3)
        axes[0].set_xticks(valid_k)

        # Highlight best k
        if len(valid_sil) > 0:
            best_idx = np.argmax(valid_sil)
            best_k = valid_k[best_idx]
            best_score = valid_sil[best_idx]
            axes[0].axvline(best_k, color="red", linestyle="--", alpha=0.7)
            axes[0].annotate(
                f"Best k={best_k}\nScore={best_score:.3f}",
                xy=(best_k, best_score),
                xytext=(best_k + 0.3, best_score + 0.01),
                arrowprops=dict(arrowstyle="->", color="red"),
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8),
            )

    # Plot 2: Gap Statistic
    if len(axes) > 1:
        gap_values = [m.get("gap_statistic", np.nan) for m in all_metrics]
        gap_stds = [m.get("gap_std", np.nan) for m in all_metrics]
        has_gap = not all(np.isnan(gap_values))

        if has_gap:
            valid_indices = ~np.isnan(gap_values)
            valid_k = np.array(k_values)[valid_indices]
            valid_gap = np.array(gap_values)[valid_indices]
            valid_gap_std = np.array(gap_stds)[valid_indices]

            axes[1].errorbar(
                valid_k,
                valid_gap,
                yerr=valid_gap_std,
                fmt="o-",
                linewidth=2,
                markersize=6,
                color="green",
                capsize=5,
            )
            axes[1].set_xlabel("Number of Clusters (k)")
            axes[1].set_ylabel("Gap Statistic")
            axes[1].set_title("Gap Statistic Analysis")
            axes[1].grid(True, alpha=0.3)
            axes[1].set_xticks(valid_k)
        else:
            # Plot group sizes as bar chart instead
            axes[1].set_title("Group Sizes by k")
            axes[1].set_xlabel("Number of Clusters (k)")
            axes[1].set_ylabel("Participants per Group")

            # Create grouped bar chart
            width = 0.8 / max(k_values) if k_values else 0.1
            for i, metrics in enumerate(all_metrics):
                k = metrics["k"]
                group_sizes = metrics.get("group_sizes", [])
                if group_sizes:
                    positions = [
                        k + j * width - width * len(group_sizes) / 2
                        for j in range(len(group_sizes))
                    ]
                    axes[1].bar(
                        positions,
                        group_sizes,
                        width=width,
                        alpha=0.7,
                        label=f"k={k}" if len(all_metrics) <= 5 else None,
                    )

            if len(all_metrics) <= 5:
                axes[1].legend()
            axes[1].grid(True, alpha=0.3)

    plt.tight_layout()

    # Save plot
    os.makedirs(output_dir, exist_ok=True)
    output_file = op.join(output_dir, "cluster_validation_summary.png")
    plt.savefig(output_file, dpi=dpi, bbox_inches="tight")
    print(f"Validation plot saved to: {output_file}")

    # Also save as PDF
    pdf_file = op.join(output_dir, "cluster_validation_summary.pdf")
    plt.savefig(pdf_file, bbox_inches="tight")
    print(f"PDF version saved to: {pdf_file}")

    plt.show()

    return output_file


def create_summary_table(all_metrics, output_dir):
    """Create a summary table of all results"""
    print("Creating summary table...")

    summary_file = op.join(output_dir, "cluster_validation_summary.txt")

    with open(summary_file, "w") as f:
        f.write("Cluster Validation Summary\n")
        f.write("=" * 50 + "\n\n")

        f.write(
            f"{'k':<3} {'Method':<12} {'Silhouette':<12} {'Gap Stat':<12} {'Group Sizes':<15}\n"
        )
        f.write("-" * 60 + "\n")

        for metrics in all_metrics:
            k = metrics["k"]
            method = metrics.get("method", "unknown")
            silhouette = metrics.get("silhouette", np.nan)
            gap_stat = metrics.get("gap_statistic", np.nan)
            group_sizes = metrics.get("group_sizes", [])

            sil_str = f"{silhouette:.3f}" if not np.isnan(silhouette) else "N/A"
            gap_str = f"{gap_stat:.3f}" if not np.isnan(gap_stat) else "N/A"
            sizes_str = str(group_sizes) if group_sizes else "N/A"

            f.write(
                f"{k:<3} {method:<12} {sil_str:<12} {gap_str:<12} {sizes_str:<15}\n"
            )

        # Add recommendations
        silhouette_scores = [m.get("silhouette", np.nan) for m in all_metrics]
        gap_stats = [m.get("gap_statistic", np.nan) for m in all_metrics]

        f.write("\nRecommendations:\n")

        # Silhouette recommendation
        if not all(np.isnan(silhouette_scores)):
            valid_scores = [
                (i, s) for i, s in enumerate(silhouette_scores) if not np.isnan(s)
            ]
            if valid_scores:
                best_idx, best_score = max(valid_scores, key=lambda x: x[1])
                best_k = all_metrics[best_idx]["k"]
                f.write(f"  Silhouette-max: k={best_k} (score: {best_score:.3f})\n")

        # Gap statistic recommendation (simplified Tibshirani rule)
        if not all(np.isnan(gap_stats)):
            # Find k with maximum gap statistic as approximation
            valid_gaps = [(i, g) for i, g in enumerate(gap_stats) if not np.isnan(g)]
            if valid_gaps:
                best_gap_idx, best_gap = max(valid_gaps, key=lambda x: x[1])
                best_gap_k = all_metrics[best_gap_idx]["k"]
                f.write(f"  Gap statistic max: k={best_gap_k} (gap: {best_gap:.3f})\n")

    print(f"Summary table saved to: {summary_file}")
    return summary_file


def main():
    """Main function"""
    args = get_parser().parse_args()

    # Set default output directory
    if args.output_dir is None:
        args.output_dir = op.join(args.results_dir, "validation_plots")

    print("Cluster Validation Plotting Tool")
    print("=" * 40)
    print(f"Results directory: {args.results_dir}")
    print(f"Output directory: {args.output_dir}")
    print(f"K range: {args.k_min} to {args.k_max}")

    try:
        # Collect all results
        all_metrics = collect_all_results(args.results_dir, args.k_min, args.k_max)

        if not all_metrics:
            print("No valid results found!")
            return 1

        # Create plots
        plot_file = plot_validation_metrics(
            all_metrics, args.output_dir, figsize=args.figsize, dpi=args.dpi
        )

        # Create summary table
        summary_file = create_summary_table(all_metrics, args.output_dir)

        print("\nSUCCESS!")
        print(f"Processed {len(all_metrics)} cluster solutions")
        print(f"Plots saved to: {args.output_dir}")

        return 0

    except Exception as e:
        print(f"ERROR: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
